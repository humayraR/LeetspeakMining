{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3768a59aead74b9fba23d32288f767dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "builder = load_dataset_builder(\"SocialGrep/one-million-reddit-jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "\n",
    "get_dataset_split_names(\"SocialGrep/one-million-reddit-jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6990b469e9493e917701e3f71ba95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8df14f127af4100bc2ad7dd390a2588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd12c61b7aa34447932e4fd723064d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60830cac478b4bb3a9521ced55ccb243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SocialGrep/one-million-reddit-jokes\", split=\"train[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'id', 'subreddit.id', 'subreddit.name', 'subreddit.nsfw', 'created_utc', 'permalink', 'domain', 'url', 'selftext', 'title', 'score'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   type            1000000 non-null  object\n",
      " 1   id              1000000 non-null  object\n",
      " 2   subreddit.id    1000000 non-null  object\n",
      " 3   subreddit.name  1000000 non-null  object\n",
      " 4   subreddit.nsfw  1000000 non-null  bool  \n",
      " 5   created_utc     1000000 non-null  int64 \n",
      " 6   permalink       1000000 non-null  object\n",
      " 7   domain          1000000 non-null  object\n",
      " 8   url             4472 non-null     object\n",
      " 9   selftext        995485 non-null   object\n",
      " 10  title           1000000 non-null  object\n",
      " 11  score           1000000 non-null  int64 \n",
      "dtypes: bool(1), int64(2), object(9)\n",
      "memory usage: 84.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../reddit/one-million-reddit-jokes.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['type', 'id', 'subreddit.id', 'subreddit.name', 'subreddit.nsfw', 'created_utc', 'permalink', 'domain', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My corona is covered with foreskin so it is no...</td>\n",
       "      <td>I am soooo glad I'm not circumcised!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's called Google Sheets.</td>\n",
       "      <td>Did you know Google now has a platform for rec...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The vacuum doesn't snore after sex.\\n\\n&amp;amp;#x...</td>\n",
       "      <td>What is the difference between my wife and my ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oo..lala...</td>\n",
       "      <td>What did the French man say to the attractive ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yo momma's so fat, that when she went to the z...</td>\n",
       "      <td>Yo Mama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999992</th>\n",
       "      <td>Q: What do you call a lawyer who has gone bad?...</td>\n",
       "      <td>BAD LAWYER</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999994</th>\n",
       "      <td>Supposedly she had to rush the delivery!</td>\n",
       "      <td>Did you hear about the FedEx lady who had a ba...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>*zyan malik or whatever leaves 1d.  \\n*Kayne W...</td>\n",
       "      <td>With Zyan Malik leaving 1D..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>I'll be Bach</td>\n",
       "      <td>What did Arnold Schwarzenegger say when invite...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>So a moth goes into a podiatrists office.\\n\\n\\...</td>\n",
       "      <td>The Moth Joke</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578637 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 selftext  \\\n",
       "0       My corona is covered with foreskin so it is no...   \n",
       "1                              It's called Google Sheets.   \n",
       "2       The vacuum doesn't snore after sex.\\n\\n&amp;#x...   \n",
       "7                                             Oo..lala...   \n",
       "10      Yo momma's so fat, that when she went to the z...   \n",
       "...                                                   ...   \n",
       "999992  Q: What do you call a lawyer who has gone bad?...   \n",
       "999994         Supposedly she had to rush the delivery!     \n",
       "999995  *zyan malik or whatever leaves 1d.  \\n*Kayne W...   \n",
       "999997                                       I'll be Bach   \n",
       "999998  So a moth goes into a podiatrists office.\\n\\n\\...   \n",
       "\n",
       "                                                    title  score  \n",
       "0                    I am soooo glad I'm not circumcised!      2  \n",
       "1       Did you know Google now has a platform for rec...      9  \n",
       "2       What is the difference between my wife and my ...     15  \n",
       "7       What did the French man say to the attractive ...      2  \n",
       "10                                                Yo Mama      0  \n",
       "...                                                   ...    ...  \n",
       "999992                                         BAD LAWYER      3  \n",
       "999994  Did you hear about the FedEx lady who had a ba...      2  \n",
       "999995                       With Zyan Malik leaving 1D..      0  \n",
       "999997  What did Arnold Schwarzenegger say when invite...      0  \n",
       "999998                                      The Moth Joke     87  \n",
       "\n",
       "[578637 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['selftext'][3])\n",
    "df = df[df['selftext']!='[removed]']\n",
    "df = df[df['selftext']!='[deleted]']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../reddit/reddit-jokes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 578637 entries, 0 to 999998\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   selftext  574122 non-null  object\n",
      " 1   title     578637 non-null  object\n",
      " 2   score     578637 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 17.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119178 entries, 0 to 119177\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   keyword      119178 non-null  object\n",
      " 1   main_tweet   119178 non-null  object\n",
      " 2   main_likes   119178 non-null  int64 \n",
      " 3   reply        119178 non-null  object\n",
      " 4   reply_likes  119178 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "print(\"\\nTraining Dataset:\")\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeetSpeak processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeetWord List:['He11o', 'W0rld']\n",
      "LeetWord: He11o\n",
      "Candidate list: ['hello', 'hero', 'hecho']\n",
      "Possible Substitutions and counts: {'L': {'1': 2}, 'C': {'1': 1}, 'H': {'1': 1}}\n",
      "LeetWord: W0rld\n",
      "Candidate list: ['world']\n",
      "Possible Substitutions and counts: {'O': {'0': 1}}\n"
     ]
    }
   ],
   "source": [
    "import LeetMining as lm\n",
    "lm.setupLeetDict()\n",
    "# lm.processTextInput(\"He11o W0rld !!\")\n",
    "# print(lm.getLeetDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# SMALL TEST WITH LEET CODE: Run Roberta and leet decode algorithm on train data to get analysis post leet substitution\n",
    "testLeet = {'keyword':[], 'reply':[], 'leetWords':[], 'candidates':[]}\n",
    "\n",
    "for i in trange(100):\n",
    "    testLeet['keyword'].append(train['keyword'].values[i])\n",
    "    testLeet['reply'].append(train['reply'].values[i])\n",
    "\n",
    "    leetWords = lm.getLeetWordList(train['reply'].values[i])\n",
    "    testLeet['leetWords'].append(leetWords)\n",
    "    candidates = []\n",
    "    for lword in leetWords:\n",
    "        possibleMatches = lm.getMatchList(lword)\n",
    "        candidates.append(possibleMatches)\n",
    "        possibleSubs = lm.getPossibleSubstitutions(lword, possibleMatches)\n",
    "        lm.updateLeetDict(possibleSubs)\n",
    "    testLeet['candidates'].append(candidates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>reply</th>\n",
       "      <th>leetWords</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>You'll be degrading your empowerment and dissi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Russia is the one with a uniquely bad NAZI pro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weather</td>\n",
       "      <td>be thankful thats all you got , we got tornado...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Queen Elizabeth</td>\n",
       "      <td>What a surprise - not</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vaccine</td>\n",
       "      <td>No, actually the doctors hope that ‚Äúyou‚Äôll ‚Äú b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword                                              reply  \\\n",
       "0         Dogecoin  You'll be degrading your empowerment and dissi...   \n",
       "1          Ukraine  Russia is the one with a uniquely bad NAZI pro...   \n",
       "2          weather  be thankful thats all you got , we got tornado...   \n",
       "3  Queen Elizabeth                              What a surprise - not   \n",
       "4          Vaccine  No, actually the doctors hope that ‚Äúyou‚Äôll ‚Äú b...   \n",
       "\n",
       "  leetWords candidates  \n",
       "0        []         []  \n",
       "1        []         []  \n",
       "2        []         []  \n",
       "3        []         []  \n",
       "4        []         []  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLeet_df = pd.DataFrame(testLeet)\n",
    "testLeet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a surprise - not'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLeet_df['reply'].values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>reply</th>\n",
       "      <th>leetWords</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>what the fuck b*tc**n is legal tender in el sa...</td>\n",
       "      <td>[b*tc**n]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>World Cup</td>\n",
       "      <td>So what about the 1000s England has killed and...</td>\n",
       "      <td>[1000s]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Canada need to Pay attention to weak peoples a...</td>\n",
       "      <td>[160Vanderhoof]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>I‚Äôm retweeting this again due to variants XBB....</td>\n",
       "      <td>[XBB.1.5, (origin, York), BF.7, (origin, China...</td>\n",
       "      <td>[[], [origin], [york], [], [origin], [china], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>nba</td>\n",
       "      <td>The only rookie &amp;gt;&amp;gt; Keegan Murray is Paol...</td>\n",
       "      <td>[gt;gt;]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>Wow, this might be the most valuable remaining...</td>\n",
       "      <td>[FTX(Though, that)]</td>\n",
       "      <td>[[], [that, thats]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>In case you missed it:üòÇ</td>\n",
       "      <td>[it:]</td>\n",
       "      <td>[[its, itu, ito, itt, ity, itl, ita, iti, it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>While Twitter cooks...you could commercializet...</td>\n",
       "      <td>[cooks...you, of:1, BusinessNews2]</td>\n",
       "      <td>[[], [ofa, offa, of, ofr, offs, offi, off, oft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>This didn‚Äôt age well , right? How many followe...</td>\n",
       "      <td>[loose#SHIB]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>TikTok</td>\n",
       "      <td>Calling early-stage founders! Awesome opportun...</td>\n",
       "      <td>[early-stage, amp;]</td>\n",
       "      <td>[[], [amp, amps]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>.  For a company with a mountain of cash and n...</td>\n",
       "      <td>[lt;, expense.(Ive, add:1), $TSLA, B/S, manage...</td>\n",
       "      <td>[[lto], [], [], [tesla, tsa, isla], [bus, bes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>That‚Äôs the way he is!!!We were honored to have...</td>\n",
       "      <td>[is!!!We]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Queen Elizabeth</td>\n",
       "      <td>If I had a nickel for every time today that so...</td>\n",
       "      <td>[out......I, 20c]</td>\n",
       "      <td>[[], [pic, usc, sic, nic, nec, arc, bac, orc, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>SARS ability of genetic recombination to produ...</td>\n",
       "      <td>[amp;endemic]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Macdonald</td>\n",
       "      <td>Look no further than  &amp;amp;  #legendesses</td>\n",
       "      <td>[amp;]</td>\n",
       "      <td>[[amp, amps]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            keyword                                              reply  \\\n",
       "7           Bitcoin  what the fuck b*tc**n is legal tender in el sa...   \n",
       "9         World Cup  So what about the 1000s England has killed and...   \n",
       "14         COVID-19  Canada need to Pay attention to weak peoples a...   \n",
       "21         COVID-19  I‚Äôm retweeting this again due to variants XBB....   \n",
       "60              nba  The only rookie &gt;&gt; Keegan Murray is Paol...   \n",
       "67         Dogecoin  Wow, this might be the most valuable remaining...   \n",
       "70        Elon Musk                            In case you missed it:üòÇ   \n",
       "73         Dogecoin  While Twitter cooks...you could commercializet...   \n",
       "76         Dogecoin  This didn‚Äôt age well , right? How many followe...   \n",
       "78           TikTok  Calling early-stage founders! Awesome opportun...   \n",
       "82            Tesla  .  For a company with a mountain of cash and n...   \n",
       "85         Dogecoin  That‚Äôs the way he is!!!We were honored to have...   \n",
       "87  Queen Elizabeth  If I had a nickel for every time today that so...   \n",
       "90         COVID-19  SARS ability of genetic recombination to produ...   \n",
       "95        Macdonald          Look no further than  &amp;  #legendesses   \n",
       "\n",
       "                                            leetWords  \\\n",
       "7                                           [b*tc**n]   \n",
       "9                                             [1000s]   \n",
       "14                                    [160Vanderhoof]   \n",
       "21  [XBB.1.5, (origin, York), BF.7, (origin, China...   \n",
       "60                                           [gt;gt;]   \n",
       "67                                [FTX(Though, that)]   \n",
       "70                                              [it:]   \n",
       "73                 [cooks...you, of:1, BusinessNews2]   \n",
       "76                                       [loose#SHIB]   \n",
       "78                                [early-stage, amp;]   \n",
       "82  [lt;, expense.(Ive, add:1), $TSLA, B/S, manage...   \n",
       "85                                          [is!!!We]   \n",
       "87                                  [out......I, 20c]   \n",
       "90                                      [amp;endemic]   \n",
       "95                                             [amp;]   \n",
       "\n",
       "                                           candidates  \n",
       "7                                                [[]]  \n",
       "9                                                [[]]  \n",
       "14                                               [[]]  \n",
       "21  [[], [origin], [york], [], [origin], [china], ...  \n",
       "60                                               [[]]  \n",
       "67                                [[], [that, thats]]  \n",
       "70  [[its, itu, ito, itt, ity, itl, ita, iti, it, ...  \n",
       "73  [[], [ofa, offa, of, ofr, offs, offi, off, oft...  \n",
       "76                                               [[]]  \n",
       "78                                  [[], [amp, amps]]  \n",
       "82  [[lto], [], [], [tesla, tsa, isla], [bus, bes,...  \n",
       "85                                               [[]]  \n",
       "87  [[], [pic, usc, sic, nic, nec, arc, bac, orc, ...  \n",
       "90                                               [[]]  \n",
       "95                                      [[amp, amps]]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testLeet_df.query('leetWords ')\n",
    "# testLeet_df[testLeet_df['leetWords'].size >0]\n",
    "# len(testLeet_df['leetWords'].values)\n",
    "\n",
    "#df['CreationDate'].str.len() -> gives length of arrays stored in cell https://stackoverflow.com/questions/41340341/how-to-determine-the-length-of-lists-in-a-pandas-dataframe-column\n",
    "testLeet_df[testLeet_df['leetWords'].str.len() >0] #just the rows with leetspeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {':': 1, '1': 1, '/': 1, '2': 3, '0': 10}, 'B': {'2': 3, '0': 1}, 'C': {}, 'D': {'2': 3}, 'E': {':': 1, '/': 1, '0': 6, '2': 1}, 'F': {':': 3, '2': 1}, 'G': {}, 'H': {':': 1, '2': 3}, 'I': {':': 1, '1': 1, '/': 1, '0': 10, '2': 1}, 'J': {}, 'K': {}, 'L': {':': 1, '2': 3}, 'M': {'2': 3}, 'N': {'2': 2, '0': 2}, 'O': {':': 1, ';': 1, '2': 1, '0': 6}, 'P': {'2': 3}, 'Q': {}, 'R': {'0': 2, '2': 3}, 'S': {';': 5, ')': 2, ':': 1, '1': 1, '0': 1, '2': 4}, 'T': {':': 1, '$': 2, '0': 2, '2': 3}, 'U': {':': 1, '/': 2, '2': 2, '0': 2}, 'V': {'2': 2}, 'W': {}, 'X': {}, 'Y': {':': 1}, 'Z': {'2': 1}}\n"
     ]
    }
   ],
   "source": [
    "print(lm.getLeetDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1, train2 = train_test_split(train, test_size=0.30)\n",
    "# train1.reset_index(drop=True)\n",
    "# train2.reset_index(drop=True)\n",
    "# print(\"Training Dataset1:\")\n",
    "# train1.info()\n",
    "# print(\"\\nTraining Dataset2:\")\n",
    "# train2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119178/119178 [3:51:32<00:00,  8.58it/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>reply</th>\n",
       "      <th>leetWords</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>You'll be degrading your empowerment and dissi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Russia is the one with a uniquely bad NAZI pro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weather</td>\n",
       "      <td>be thankful thats all you got , we got tornado...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Queen Elizabeth</td>\n",
       "      <td>What a surprise - not</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vaccine</td>\n",
       "      <td>No, actually the doctors hope that ‚Äúyou‚Äôll ‚Äú b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword                                              reply  \\\n",
       "0         Dogecoin  You'll be degrading your empowerment and dissi...   \n",
       "1          Ukraine  Russia is the one with a uniquely bad NAZI pro...   \n",
       "2          weather  be thankful thats all you got , we got tornado...   \n",
       "3  Queen Elizabeth                              What a surprise - not   \n",
       "4          Vaccine  No, actually the doctors hope that ‚Äúyou‚Äôll ‚Äú b...   \n",
       "\n",
       "  leetWords candidates  \n",
       "0        []         []  \n",
       "1        []         []  \n",
       "2        []         []  \n",
       "3        []         []  \n",
       "4        []         []  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run leet algorithm on train data to extract leet words and their substitutions\n",
    "lm.setupLeetDict() # initiate FP list\n",
    "trainLeet = {'keyword':[], 'reply':[], 'leetWords':[], 'candidates':[]}\n",
    "\n",
    "for i in trange(len(train['reply'].values)):\n",
    "    trainLeet['keyword'].append(train['keyword'].values[i])\n",
    "    trainLeet['reply'].append(train['reply'].values[i])\n",
    "\n",
    "    leetWords = lm.getLeetWordList(train['reply'].values[i])\n",
    "    trainLeet['leetWords'].append(leetWords)\n",
    "    candidates = []\n",
    "    for lword in leetWords:\n",
    "        possibleMatches = lm.getMatchList(lword)\n",
    "        candidates.append(possibleMatches)\n",
    "        possibleSubs = lm.getPossibleSubstitutions(lword, possibleMatches)\n",
    "        lm.updateLeetDict(possibleSubs)\n",
    "    trainLeet['candidates'].append(candidates)\n",
    "\n",
    "train1_result_df = pd.DataFrame(trainLeet)\n",
    "train1_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect training results - leet dict and the result table\n",
    "import json\n",
    "with open(\"twitter_leetDict.json\", \"w\") as outfile: \n",
    "    json.dump(lm.getLeetDict(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‚Äô': 1037, '8': 132, '?': 2, '*': 49, '(': 259, '7': 72, '0': 163, '3': 282, '$': 443, '\"': 201, '2': 317, '‚Äò': 41, ')': 78, '‚Äú': 151, '1': 468, '5': 167, '-': 38, ':': 88, '‚Ä¶': 208, '.': 60, ',': 1, '‚Åµ': 11, '9': 95, '4': 108, '6': 60, \"'\": 45, '{': 1, '¬£': 8, ';': 6, '!': 2, '¬ª': 2, '+': 18, '\\xa0': 3, '/': 14, '‚Äù': 5, '=': 3, '¬≤': 10, '‚Å∑': 6, '‚Å∏': 6, '`': 1, '[': 12, '_': 1, '‚Äî': 10, '^': 7, ']': 1}\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# leetDict['B']\n",
    "\n",
    "# allKeysCounts = []\n",
    "# allKeys = []\n",
    "# for key in leetDict[\"B\"].keys():\n",
    "#     allKeys.append(key)\n",
    "#     allKeysCounts.append(leetDict[\"B\"][key])\n",
    "    \n",
    "#     #print(key, leetDict[\"F\"][key])\n",
    "# f_set = np.array(leetDict[\"B\"])\n",
    "# print(f_set)\n",
    "\n",
    "# f_set_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119178 entries, 0 to 119177\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   keyword     119178 non-null  object\n",
      " 1   reply       119178 non-null  object\n",
      " 2   leetWords   119178 non-null  object\n",
      " 3   candidates  119178 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# train1_result_df.to_csv('train1_result.csv', index=False)\n",
    "# train1.to_csv('train1.csv', index=False)\n",
    "train1_result_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>reply</th>\n",
       "      <th>leetWords</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>what the fuck b*tc**n is legal tender in el sa...</td>\n",
       "      <td>[b*tc**n]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>World Cup</td>\n",
       "      <td>So what about the 1000s England has killed and...</td>\n",
       "      <td>[1000s]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Canada need to Pay attention to weak peoples a...</td>\n",
       "      <td>[160Vanderhoof]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>I‚Äôm retweeting this again due to variants XBB....</td>\n",
       "      <td>[XBB.1.5, (origin, York), BF.7, (origin, China...</td>\n",
       "      <td>[[], [origin], [york], [], [origin], [china], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>nba</td>\n",
       "      <td>The only rookie &amp;gt;&amp;gt; Keegan Murray is Paol...</td>\n",
       "      <td>[gt;gt;]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119160</th>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>It will get a top-notch care</td>\n",
       "      <td>[top-notch]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119163</th>\n",
       "      <td>Vaccine</td>\n",
       "      <td>#ErasingWomenWomen, pregnant women.People incl...</td>\n",
       "      <td>[women.People]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119167</th>\n",
       "      <td>nba</td>\n",
       "      <td>$mommie0198 I absolutely love and adore my 2 d...</td>\n",
       "      <td>[$mommie0198]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119168</th>\n",
       "      <td>weather</td>\n",
       "      <td>At least A&amp;amp;W gets it. I quit buying Coke c...</td>\n",
       "      <td>[Aamp;W, Aamp;W]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119173</th>\n",
       "      <td>Queen Elizabeth</td>\n",
       "      <td>I can glom onto this line. I just had to be su...</td>\n",
       "      <td>[(and, did):So]</td>\n",
       "      <td>[[land, wand, hand, and, nand, sand, rand, can...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26230 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword                                              reply  \\\n",
       "7               Bitcoin  what the fuck b*tc**n is legal tender in el sa...   \n",
       "9             World Cup  So what about the 1000s England has killed and...   \n",
       "14             COVID-19  Canada need to Pay attention to weak peoples a...   \n",
       "21             COVID-19  I‚Äôm retweeting this again due to variants XBB....   \n",
       "60                  nba  The only rookie &gt;&gt; Keegan Murray is Paol...   \n",
       "...                 ...                                                ...   \n",
       "119160         Dogecoin                       It will get a top-notch care   \n",
       "119163          Vaccine  #ErasingWomenWomen, pregnant women.People incl...   \n",
       "119167              nba  $mommie0198 I absolutely love and adore my 2 d...   \n",
       "119168          weather  At least A&amp;W gets it. I quit buying Coke c...   \n",
       "119173  Queen Elizabeth  I can glom onto this line. I just had to be su...   \n",
       "\n",
       "                                                leetWords  \\\n",
       "7                                               [b*tc**n]   \n",
       "9                                                 [1000s]   \n",
       "14                                        [160Vanderhoof]   \n",
       "21      [XBB.1.5, (origin, York), BF.7, (origin, China...   \n",
       "60                                               [gt;gt;]   \n",
       "...                                                   ...   \n",
       "119160                                        [top-notch]   \n",
       "119163                                     [women.People]   \n",
       "119167                                      [$mommie0198]   \n",
       "119168                                   [Aamp;W, Aamp;W]   \n",
       "119173                                    [(and, did):So]   \n",
       "\n",
       "                                               candidates  \n",
       "7                                                    [[]]  \n",
       "9                                                    [[]]  \n",
       "14                                                   [[]]  \n",
       "21      [[], [origin], [york], [], [origin], [china], ...  \n",
       "60                                                   [[]]  \n",
       "...                                                   ...  \n",
       "119160                                               [[]]  \n",
       "119163                                               [[]]  \n",
       "119167                                               [[]]  \n",
       "119168                                           [[], []]  \n",
       "119173  [[land, wand, hand, and, nand, sand, rand, can...  \n",
       "\n",
       "[26230 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print leetword rows only\n",
    "train1_result_df[train1_result_df['leetWords'].str.len() >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {':': 969, '1': 3934, '/': 401, '2': 2548, '0': 7179, '(': 571, '5': 2972, ')': 540, '$': 981, '8': 1091, '3': 1889, '6': 620, '*': 481, '9': 1480, '-': 447, '7': 728, '4': 1154, '.': 706, ';': 805, '¬£': 13, '[': 31, '¬´': 136, '\\xa0': 207, '=': 27, '{': 2, '‚Äì': 1, '!': 130, '+': 62, '¬ø': 12, '¬≤': 52, '‚Å∑': 18, '‚Å∏': 18, '_': 11, '‚ò≠': 7, '‚Äî': 51, '¬ª': 3, '#': 19, '\\u202f': 1, '‚Äû': 4, 'Ôøº': 1, '^': 14, 'üñí': 3, '%': 8, '‚Üí': 1, 'Ôºü': 1, '~': 17, ']': 19, '„ÄÇ': 1, '¬°': 1, '¬∞': 3, '‚Åµ': 18, '‚Å∂': 18, '\\u3000': 1, '‚Üì': 1, '‚Ä¢': 1, '\\u200b': 12}, 'B': {'2': 695, '0': 576, '(': 540, '/': 16, '$': 828, '1': 917, ':': 195, '3': 552, '5': 346, '9': 404, '7': 222, '=': 5, '*': 68, '6': 146, ')': 150, '8': 317, ';': 201, '‚Åµ': 29, '[': 14, '4': 287, '.': 250, '\\xa0': 39, '-': 56, '¬£': 12, '+': 20, 'Ôøº': 1, '`': 1, '!': 25, '‚Å∑': 6, '¬≤': 20, '‚Å∏': 6, '‚Äî': 16, '¬ª': 3, '#': 5, '{': 1, 'üñí': 3, '‚ô°': 1, '~': 4, ']': 1, '¬°': 2, '¬´': 1, '‚Å∂': 6, '_': 1, '^': 7, '\\u200b': 2}, 'C': {'(': 568, '/': 13, '2': 441, '$': 748, ')': 131, ':': 202, '1': 467, '3': 408, '!': 35, '0': 736, '5': 715, '9': 461, '=': 5, '*': 95, '6': 183, '‚Ä¢': 1, '.': 264, '7': 241, '8': 347, ';': 143, '‚Åµ': 26, '4': 224, '\\xa0': 37, '-': 90, '#': 11, '+': 17, '¬≤': 24, '‚Å∑': 3, '‚Å∏': 3, '[': 9, '_': 1, '%': 5, '‚Äî': 9, '¬ª': 3, '{': 1, '¬£': 6, '‚ô°': 1, '¬¥': 2, '~': 5, ']': 2, '¬¢': 1, '¬°': 5, '‚Å∂': 3, '^': 5, '\\u200b': 3}, 'D': {'2': 713, '(': 573, ')': 352, '$': 930, ':': 758, '5': 387, '1': 1202, '0': 708, '3': 680, '.': 330, '!': 86, '‚Ä∫': 6, '9': 472, '7': 232, ';': 325, '*': 110, '6': 212, '8': 310, '‚Åµ': 25, '[': 12, '4': 395, '-': 117, '\\xa0': 23, '‚Ä¢': 1, '¬£': 12, '+': 34, '/': 13, '¬≤': 29, '‚Å∑': 2, '‚Å∏': 2, '‚Äî': 20, '¬ª': 4, '#': 6, '{': 2, 'Ôøº': 1, 'üñí': 3, ']': 9, '=': 2, '‚ô°': 1, '¬¥': 3, '~': 4, '%': 2, 'Ôºå': 1, '‚Å∂': 2, '_': 1, '^': 5, '\\u200b': 3}, 'E': {':': 1081, '/': 421, '0': 5086, '2': 1900, '(': 420, '1': 3039, '4': 881, '5': 1286, ')': 740, '$': 930, '8': 766, '3': 1222, '6': 669, '*': 455, '!': 155, '9': 1262, '-': 388, ';': 676, '.': 748, '7': 549, '%': 19, '‚Ç¨': 3, '‚Åµ': 33, '¬£': 7, '[': 18, '‚ùù': 6, '\\xa0': 65, '¬´': 34, '#': 20, '+': 63, '{': 2, '‚Äì': 1, '‚Äî': 38, '`': 1, '¬ø': 12, '‚Å∑': 10, '¬≤': 24, '‚Å∏': 10, '¬ª': 3, '_': 9, 'Ôøº': 1, '^': 15, '‚Üí': 1, '=': 23, '‚ô°': 1, 'Ôºü': 1, '~': 11, ']': 31, '¬¢': 1, '¬∞': 3, '‚Å∂': 10, '\\u200b': 10}, 'F': {':': 201, '2': 343, '(': 454, '$': 797, ')': 68, '5': 163, '3': 423, '0': 248, '-': 80, '9': 146, '*': 63, '6': 114, '.': 158, '7': 93, '8': 150, ';': 192, '1': 370, '[': 11, '4': 178, '\\xa0': 38, '¬£': 10, '+': 23, '!': 24, '‚Å∑': 1, '¬≤': 28, '‚Å∏': 1, '‚Äî': 10, '¬ª': 3, '#': 1, '{': 1, '^': 8, 'üñí': 3, '=': 2, '‚ô°': 1, '¬¥': 2, '~': 5, ']': 2, '/': 3, '¬°': 1, '‚Åµ': 1, '‚Å∂': 1, '\\u200b': 1}, 'G': {'(': 445, '2': 529, ')': 155, '$': 691, ':': 333, '5': 303, '1': 1156, '0': 268, '9': 442, '3': 427, '‚Ä∫': 6, '7': 229, '*': 85, '6': 155, '8': 302, ';': 196, '‚Åµ': 24, '[': 11, '-': 60, '4': 259, '.': 165, '¬´': 34, '!': 57, '/': 11, '¬£': 8, '+': 27, '‚Å∑': 1, '¬≤': 16, '‚Å∏': 1, '%': 3, '\\xa0': 2, '‚Äî': 15, '¬ª': 3, '#': 7, '{': 1, 'Ôøº': 1, '=': 2, '‚ô°': 1, '¬¥': 2, '~': 5, ']': 2, '¬°': 3, '‚Å∂': 1, '^': 4, '\\u200b': 3}, 'H': {':': 437, '2': 581, '(': 572, '/': 105, ')': 275, '1': 1206, '$': 660, '5': 279, '3': 416, '*': 116, '0': 242, '9': 344, '.': 276, '‚Å∑': 30, '6': 146, '7': 166, '8': 302, ';': 275, '‚Åµ': 30, '[': 10, '-': 80, '4': 238, '\\xa0': 26, '=': 9, '!': 52, '+': 28, '¬ø': 12, '¬≤': 16, '‚Å∏': 7, '‚Äî': 12, '¬ª': 3, '{': 1, '\\u202f': 1, 'Ôøº': 1, 'üñí': 3, '¬£': 8, '√ó': 1, '‚ô°': 1, '¬¥': 2, '~': 5, ']': 2, '¬°': 6, '#': 4, '‚Å∂': 7, '_': 1, '^': 4, '\\u200b': 4}, 'I': {':': 877, '1': 2748, '/': 382, '0': 4274, '2': 1868, '(': 394, '4': 901, '5': 1411, ')': 340, '$': 776, '8': 860, '3': 1289, '6': 614, '*': 275, '!': 127, '9': 1203, '-': 523, '.': 584, ';': 519, '7': 499, '%': 16, '‚Åµ': 26, '[': 16, '\\xa0': 68, '‚Äì': 1, '¬£': 10, '+': 58, '`': 1, '¬ø': 12, '¬≤': 36, '‚Å∑': 3, '‚Å∏': 3, '_': 16, '‚Äî': 34, '¬ª': 3, '#': 18, 'Ôøº': 1, '=': 15, 'Ôºü': 1, '~': 12, ']': 14, '¬∞': 3, '‚Å∂': 3, '^': 6, '\\u200b': 2}, 'J': {'(': 194, '$': 416, '3': 258, '5': 87, '9': 94, '7': 79, '2': 219, '6': 59, '1': 186, '8': 110, '[': 3, '4': 115, ';': 3, ')': 24, '0': 48, '¬£': 10, '*': 22, '+': 8, '-': 13, ':': 13, '¬≤': 8, '‚Äî': 3, '¬ª': 3, '.': 9, 'üñí': 3, '!': 1, '#': 1, '~': 2, '^': 4, '\\u200b': 1}, 'K': {'(': 408, '$': 616, ':': 315, '5': 228, '0': 369, '3': 450, '‚Ä∫': 6, '9': 246, '6': 135, '2': 334, '.': 229, ')': 154, '%': 6, '7': 138, '8': 250, ';': 110, '1': 347, '‚Åµ': 27, '[': 2, '4': 211, '-': 53, '\\xa0': 55, '!': 38, '=': 6, '+': 26, '*': 79, '¬£': 6, '/': 6, '‚Å∑': 4, '¬≤': 8, '‚Å∏': 4, '‚Äî': 9, '¬´': 3, '¬ª': 3, '#': 3, '{': 1, 'üñí': 3, '‚ô°': 1, '¬¥': 2, '~': 2, ']': 3, '¬°': 5, '‚Å∂': 4, '_': 1, '^': 1, '\\u200b': 3}, 'L': {':': 733, '2': 872, '(': 745, '/': 93, ')': 313, '$': 919, '1': 1503, '5': 740, '3': 751, '0': 1290, '9': 503, '‚Ä∫': 6, '.': 403, '#': 17, '6': 223, '7': 247, '8': 344, ';': 364, '!': 111, '-': 178, '‚Åµ': 27, '[': 7, '4': 334, '‚ùû': 3, '*': 228, '\\xa0': 52, '¬´': 34, '\\u2005': 1, '¬£': 13, '+': 33, ']': 11, '¬ø': 12, '¬≤': 16, '‚Å∑': 4, '‚Å∏': 4, '_': 6, '‚Äî': 24, '¬ª': 3, '{': 1, 'Ôøº': 1, 'üñí': 3, '√ó': 3, '=': 3, '‚ô°': 1, '¬¥': 2, '~': 3, '%': 2, '‚Å∂': 4, '^': 6, '\\u200b': 2}, 'M': {'2': 774, '(': 485, ')': 191, '1': 1575, '$': 840, ':': 410, '5': 402, '3': 687, '0': 748, '9': 504, '7': 252, '*': 94, '6': 196, '.': 259, '8': 343, ';': 435, '‚Åµ': 27, '[': 8, '4': 304, '/': 45, '¬´': 34, '\\xa0': 48, '-': 117, '¬£': 4, '+': 36, '=': 2, '!': 53, '‚Å∑': 4, '¬≤': 16, '‚Å∏': 4, '%': 5, '‚Äî': 17, '_': 2, '¬ª': 3, '#': 6, '{': 1, 'Ôøº': 1, '‚ô°': 1, '~': 5, ']': 4, '¬°': 3, '‚Å∂': 4, '^': 1, '\\u200b': 3}, 'N': {'2': 906, '0': 1864, '(': 552, '/': 58, ')': 346, '$': 997, ':': 684, '.': 724, '5': 694, '1': 1279, '3': 716, '9': 400, '-': 224, ';': 390, '6': 338, '7': 217, '8': 347, '[': 10, '4': 401, '*': 170, '\\xa0': 51, '¬ª': 37, '!': 93, '#': 16, '¬£': 17, '+': 38, '`': 1, ']': 9, '‚Å∑': 4, '¬≤': 20, '‚Å∏': 4, '_': 2, '‚Äî': 21, '{': 1, '\\u202f': 1, 'Ôøº': 1, '=': 4, '%': 8, '‚ô°': 1, '¬¥': 2, '~': 5, '‚Åµ': 4, '‚Å∂': 4, 'ÔºÅ': 1, '^': 6, '\\u200b': 1}, 'O': {':': 568, ';': 756, '2': 2032, '0': 4602, '(': 497, '1': 3001, '4': 991, '5': 1850, '$': 589, '8': 746, '3': 1388, '.': 690, '/': 400, '6': 532, '*': 448, ')': 444, '9': 1186, '7': 517, '%': 18, '-': 767, '¬£': 11, '[': 26, '‚ùù': 3, '\\xa0': 55, '¬ª': 37, '{': 2, '‚Äì': 1, '!': 96, '+': 55, '¬ø': 12, '‚Å∑': 15, '¬≤': 20, '‚Å∏': 15, '_': 10, '=': 22, '‚Äî': 39, '#': 17, '‚Äû': 4, '¬¥': 2, '¬•': 1, 'Ôºü': 1, '~': 11, ']': 14, '‚ô°': 1, '‚Ç¨': 1, '¬°': 1, '‚Åµ': 15, '‚Å∂': 15, '\\u3000': 1, '‚Üì': 1, '‚Ä¢': 1, '^': 6, '\\u200b': 7}, 'P': {'2': 552, '(': 473, '/': 19, '$': 694, '.': 359, '5': 268, '1': 666, '3': 497, '0': 806, '9': 383, '7': 208, '=': 2, '*': 121, '6': 161, ')': 151, '8': 234, ';': 119, '-': 91, ':': 266, '[': 14, '4': 239, '¬´': 34, '\\xa0': 11, '¬£': 4, '+': 36, '!': 14, '‚Å∑': 1, '¬≤': 20, '‚Å∏': 1, '%': 3, '‚Äî': 16, '_': 1, '¬ª': 3, '#': 5, '{': 1, '^': 5, 'üñí': 3, '‚ô°': 1, '¬¥': 2, '~': 6, ']': 1, '‚Åµ': 1, '‚Å∂': 1, '\\u200b': 2}, 'Q': {'(': 107, '2': 55, ':': 78, '[': 2, '3': 54, '¬´': 34, '1': 34, ')': 18, '-': 8, '.': 6, '+': 1, '*': 7, '9': 23, '¬≤': 4, '0': 5, '‚Äî': 2, '4': 10, '¬ª': 3, '$': 5, '5': 3, '=': 1, '6': 1, ';': 1, '7': 1}, 'R': {'0': 1648, '2': 870, '(': 629, '/': 86, ')': 284, '$': 966, '1': 1534, ':': 507, '5': 729, '3': 520, '9': 350, '-': 252, ';': 316, '*': 184, '6': 202, '8': 220, '.': 461, '7': 179, '[': 6, '4': 304, '‚ùû': 6, '\\xa0': 96, '¬´': 102, '!': 78, '#': 13, '+': 41, 'Ôøº': 2, '¬≤': 13, '‚Å∑': 11, '‚Å∏': 11, '_': 3, '%': 13, '‚Äî': 16, '¬ª': 4, '{': 2, 'üñí': 3, '¬£': 9, '=': 3, '‚ô°': 1, 'Ôºâ': 1, '~': 2, ']': 6, 'Ôºå': 1, '‚Åµ': 11, '‚Å∂': 11, '^': 4, '\\u200b': 6}, 'S': {';': 4642, ')': 1220, ':': 2338, '1': 1858, '0': 1758, '2': 1194, '(': 747, '3': 1132, '$': 1374, '.': 888, '5': 768, '*': 510, '!': 204, '9': 747, '4': 665, '6': 376, '7': 399, '=': 14, '-': 305, '/': 61, '%': 12, '8': 588, '+': 63, '‚Åµ': 28, '[': 12, '‚´Ø': 3, '¬´': 111, '\\xa0': 65, '{': 3, '#': 11, '¬£': 9, '‚Äî': 34, 'Ôøº': 2, '‚Ä¢': 1, ']': 31, '¬ø': 13, '¬≤': 41, '‚Å∑': 5, '‚Å∏': 5, '_': 6, '¬ª': 3, '\\u202f': 2, '¬°': 7, 'ÔºÅ': 2, 'üñí': 3, '‚Üí': 2, '~': 10, '‚áæ': 1, '‚ô°': 1, '¬¥': 3, '‚Äì': 1, '}': 1, '‚Å∂': 5, '\\u3000': 1, '^': 4, '\\u200b': 5}, 'T': {':': 798, '$': 1526, '0': 803, '2': 1009, '(': 742, '/': 65, ')': 431, '1': 1420, '5': 453, '3': 875, '*': 183, '!': 112, '.': 495, '9': 469, '-': 167, '7': 226, ';': 538, '6': 229, '8': 405, '‚Åµ': 27, '[': 15, '4': 426, '\\xa0': 51, '+': 46, '¬£': 12, 'Ôøº': 2, '¬ø': 12, '¬≤': 20, '‚Å∑': 4, '‚Å∏': 4, '=': 7, '‚Äî': 21, '_': 5, '¬´': 3, '¬ª': 3, '#': 9, '{': 1, '\\u202f': 1, '‚Äû': 4, '^': 4, 'üñí': 3, '√ó': 1, '‚ô°': 1, '~': 8, ']': 10, '¬°': 1, '‚Å∂': 4, '‚Üì': 1, '‚Ä¢': 1, '\\u200b': 2}, 'U': {':': 384, '/': 302, '2': 1293, '0': 3096, '1': 2530, '5': 894, ')': 200, '$': 483, '8': 350, '3': 593, '6': 241, '*': 246, '9': 606, ';': 220, '7': 270, '4': 546, '.': 324, '-': 440, '¬£': 5, '(': 116, '\\xa0': 46, '‚Äì': 1, '¬¥': 11, '+': 43, '`': 1, '!': 49, '_': 8, '¬ø': 12, '‚Å∑': 13, '¬≤': 20, '‚Å∏': 13, '‚ò≠': 7, '‚Äî': 24, '#': 12, 'Ôøº': 1, '%': 13, '[': 12, 'Ôºü': 1, '~': 10, ']': 6, '=': 9, '¬∞': 3, '^': 6, '¬´': 1, '‚Åµ': 13, '‚Å∂': 13, '\\u200b': 1}, 'V': {'2': 208, '(': 398, '$': 317, ')': 65, ':': 171, '3': 291, '0': 83, '5': 133, '9': 346, '7': 146, '6': 133, '.': 165, '1': 186, '%': 2, '8': 245, ';': 253, '‚Åµ': 26, '[': 4, '4': 151, '\\xa0': 76, '¬´': 34, '!': 9, '*': 47, '+': 18, '-': 35, '/': 3, '‚Å∑': 3, '¬≤': 8, '‚Å∏': 3, '‚Äî': 5, '¬ª': 3, '#': 2, '{': 1, '_': 1, 'üñí': 3, '‚ô°': 1, '¬¥': 2, '~': 4, '‚Å∂': 3, '\\u200b': 2}, 'W': {'(': 629, '2': 569, ')': 122, '$': 381, '1': 1012, '3': 374, '.': 149, '5': 179, '0': 331, '‚Ä∫': 6, '9': 265, ':': 170, '*': 74, '6': 92, '7': 113, '8': 214, ';': 67, '‚Åµ': 24, '[': 10, '4': 198, '-': 45, '¬£': 6, '+': 19, '!': 33, '/': 19, '¬≤': 12, '‚Å∑': 1, '‚Å∏': 1, '%': 3, '‚Äî': 7, '\\xa0': 9, '¬ª': 3, '#': 3, '{': 1, 'Ôøº': 1, '=': 3, '‚ô°': 1, '¬¥': 2, '~': 4, '¬°': 4, '‚Å∂': 1, '^': 1, '\\u200b': 1}, 'X': {'(': 105, '5': 50, '3': 61, ')': 72, '2': 98, ';': 2, ':': 118, '[': 2, '4': 78, '1': 34, '0': 107, '¬´': 34, '$': 63, '6': 10, '.': 18, '8': 2, '-': 43, '+': 16, '*': 25, '9': 24, '!': 2, '/': 9, '¬≤': 4, '‚Äî': 5, '¬ª': 3, '#': 3, '^': 1, '%': 4}, 'Y': {':': 505, '(': 280, '5': 419, '1': 450, '$': 357, ')': 428, '8': 327, '3': 578, '-': 120, '9': 475, '/': 167, '0': 700, '4': 363, '.': 156, '6': 175, '2': 614, '7': 183, '%': 14, ';': 431, '[': 7, '‚´Ø': 3, '*': 94, '¬ª': 71, '\\xa0': 10, '=': 6, '‚Äì': 2, '!': 60, '¬£': 6, '‚Äî': 11, '+': 33, '¬ø': 13, '¬≤': 8, '#': 10, '{': 1, '\\u202f': 4, '‚áæ': 1, '‚ô°': 1, 'Ôºü': 1, '~': 5, ']': 18, '¬°': 1, '_': 2, '}': 1, '^': 1}, 'Z': {'2': 216, '(': 122, '$': 168, '5': 82, '-': 19, ')': 35, '0': 17, ';': 6, '1': 138, '7': 20, '3': 191, ':': 124, '4': 75, '6': 25, '.': 68, '8': 35, '¬£': 4, '+': 13, '9': 56, '/': 4, '¬≤': 4, '‚Å∑': 1, '‚Å∏': 1, '‚Äî': 3, '¬ª': 3, '#': 1, '*': 20, '!': 1, '~': 2, '‚Åµ': 1, '‚Å∂': 1, '^': 1}}\n"
     ]
    }
   ],
   "source": [
    "print(lm.getLeetDict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
